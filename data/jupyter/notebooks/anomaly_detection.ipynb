{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbb9670",
   "metadata": {},
   "source": [
    "This notebook will read data from the InfluxDB and use it to train the TCN model. Then it will use this model to continuously output an anomaly score back to the InfluxDB which can be output there via bucket-->anomalies-->scores-->tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5104a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Enter the information from the InfluxDB here.\n",
    "###In particular, be sure not to remove the outer quotation marks for the bucket and the measurement variables.\n",
    "###Fields need to be given as lists\n",
    "\n",
    "organisation = \"manubrain\"\n",
    "bucket = '\"sinus\"'\n",
    "msm = '\"anomalie_detection\"'\n",
    "fields = ['field_0','field_1','field_2','field_3','field_4','field_5','field_6','field_7','field_8','field_9']\n",
    "\n",
    "###Give the time_window for how far we should go into the past to construct the training data\n",
    "time_window = \"1h\"\n",
    "\n",
    "###Give the desired number of training epochs of the method\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a249b5d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/jovyan/detect\r\n",
      "\u001b[31mERROR: file:///home/jovyan/detect does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "!pip install -e /home/jovyan/detect/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e7347d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".   bin   dev\t      etc   lib    lib64   media  opt\troot  sbin  sys  usr\r\n",
      "..  boot  .dockerenv  home  lib32  libx32  mnt\t  proc\trun   srv   tmp  var\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../dev -a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b3228f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mb_detect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmb_detect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sklearn_dect\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmb_detect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeep\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtcn_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TCN\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mb_detect'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from mb_detect.models.classic import sklearn_dect\n",
    "from mb_detect.models.deep.tcn_model import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e66618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "import influxdb_client, os, time\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://influxdb:8086\"\n",
    "token = os.environ.get(\"INFLUXDB_TOKEN\")\n",
    "client = influxdb_client.InfluxDBClient(url=url, token=token, org=organisation)\n",
    "query_api = client.query_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b36ad",
   "metadata": {},
   "source": [
    "The fields need to be brought in the correct form for the query api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6815cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_query = ''\n",
    "\n",
    "if len(fields) == 1:\n",
    "    field_query = 'r[\"_field\"] == ' + str(fields[0])\n",
    "    \n",
    "else:\n",
    "\n",
    "    for i in range(len(fields)-1):\n",
    "        field_query = field_query + 'r[\"_field\"] == \"' + fields[i] + '\"' + ' or '\n",
    "        \n",
    "    field_query = field_query + 'r[\"_field\"] == \"' + fields[-1] + '\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9d1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query script for loading the data from the influxDB\n",
    "\n",
    "def data_loader(bucket,msm,field_query,time_window=\"10s\"):\n",
    "\n",
    "    query = 'from(bucket:{})\\\n",
    "    |> range(start: -{})\\\n",
    "    |> filter(fn:(r) => r._measurement == {})\\\n",
    "    |> filter(fn:(r) => {})\\\n",
    "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\\\n",
    "    '.format(bucket,time_window,msm,field_query)\n",
    "\n",
    "    data = query_api.query_data_frame(org=\"manubrain\", query=query)\n",
    "    \n",
    "    ###We need to rename to columns to the name 'value_nr' in order to be handable for the method\n",
    "    rename = {}\n",
    "    [rename.update({c:c.replace(\"field\", \"value\")}) for c in data.columns if c.startswith(\"field\")]\n",
    "    data.rename(columns=rename, inplace=True)\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15aa2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_loader(bucket,msm,field_query,time_window=time_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cac0da",
   "metadata": {},
   "source": [
    "Train the TCN model on the chosen train data with the chosen number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c17ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[[c for c in train_data.columns if c.startswith(\"value\")]]\n",
    "model = TCN(X.shape[1])\n",
    "model.fit(X,epochs=epochs) #turn up epochs for better learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6548483",
   "metadata": {},
   "source": [
    "The model is used to make predictions on the next data point based on the last 5 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c22daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_tcn(model):\n",
    "    import numpy as np\n",
    "    test_data = data_loader(bucket,msm,field_query,time_window=\"10s\")\n",
    "    test_data = test_data[[c for c in train_data.columns if c.startswith(\"value\")]]\n",
    "    tx = torch.Tensor(test_data.to_numpy()).T\n",
    "    history = tx[...,-6:-1]\n",
    "    predict_target = tx[...,-1].numpy()\n",
    "    predict_model = model.forward(history).detach().numpy().flatten()\n",
    "    predict_error = np.linalg.norm(predict_target-predict_model)\n",
    "    return [predict_error,predict_target,predict_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92475e",
   "metadata": {},
   "source": [
    "Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e48043",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m point \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m   Point(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manomalies\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;241m.\u001b[39mtag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomaly_score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;241m.\u001b[39mfield(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtcn\u001b[39m\u001b[38;5;124m\"\u001b[39m, predict_error)\n\u001b[1;32m     10\u001b[0m   )\n\u001b[1;32m     11\u001b[0m write_api\u001b[38;5;241m.\u001b[39mwrite(bucket\u001b[38;5;241m=\u001b[39mbucket\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), org\u001b[38;5;241m=\u001b[39morganisation, record\u001b[38;5;241m=\u001b[39mpoint)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "while True:\n",
    "    predictions = make_predictions_tcn(model)\n",
    "    predict_error = predictions[0]\n",
    "\n",
    "    point = (\n",
    "      Point(\"anomalies\")\n",
    "      .tag(\"scores\", 'anomaly_score')\n",
    "      .field(\"tcn\", predict_error)\n",
    "      )\n",
    "    write_api.write(bucket=bucket.replace('\"',\"\"), org=organisation, record=point)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc8a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
